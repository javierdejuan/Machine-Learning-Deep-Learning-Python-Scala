{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de sentimientos con reviews de productos de Amazon España (opcional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si has hecho ya el ejercicio de web scraping con `Requests` y `BeautifulSoup` habrás visto cómo extraer datos de una página web.\n",
    "\n",
    "El dataset que utilizarás en este ejercicio (que no es obligatorio entregar) lo he generado utilizando `Scrapy` y `BeautifulSoup`, y contiene unas $700.000$ entradas con dos columnas: el número de estrellas dadas por un usuario a un determinado producto y el comentario sobre dicho producto; exactamente igual que en el ejercico de scraping.\n",
    "\n",
    "Ahora, tu objetivo es utilizar técnicas de procesamiento de lenguaje natural para hacer un clasificador que sea capaz de distinguir (¡y predecir!) si un comentario es positivo o negativo.\n",
    "\n",
    "Es un ejercicio MUY complicado, más que nada porque versa sobre técnicas que no hemos visto en clase. Así que si quieres resolverlo, te va a tocar estudiar y *buscar por tu cuenta*; exactamente igual que como sería en un puesto de trabajo. Dicho esto, daré un par de pistas:\n",
    "\n",
    "+ El número de estrellas que un usuario da a un producto es el indicador de si a dicho usuario le ha gustado el producto o no. Una persona que da 5 estrellas (el máximo) a un producto probablemente esté contento con él, y el comentario será por tanto positivo; mientras que cuando una persona da 1 estrella a un producto es porque no está satisfecha... \n",
    "+ Teniendo el número de estrellas podríamos resolver el problema como si fuera de regresión; pero vamos a establecer una regla para convertirlo en problema de clasificación: *si una review tiene 4 o más estrellas, se trata de una review positiva; y será negativa si tiene menos de 4 estrellas*. Así que probablemente te toque transformar el número de estrellas en otra variable que sea *comentario positivo/negativo*.\n",
    "\n",
    "Y... poco más. Lo más complicado será convertir el texto de cada review en algo que un clasificador pueda utilizar y entender (puesto que los modelos no entienden de palabras, sino de números). Aquí es donde te toca investigar las técnicas para hacerlo. El ejercicio se puede conseguir hacer, y obtener buenos resultados, utilizando únicamente Numpy, pandas y Scikit-Learn; pero siéntete libre de utilizar las bibliotecas que quieras.\n",
    "\n",
    "Ahora escribiré una serie de *keywords* que probablemente te ayuden a saber qué buscar:\n",
    "\n",
    "`bag of words, tokenizer, tf, idf, tf-idf, sklearn.feature_extraction, scipy.sparse, NLTK (opcional), stemmer, lemmatizer, stop-words removal, bigrams, trigrams`\n",
    "\n",
    "No te desesperes si te encuentras muy perdido/a y no consigues sacar nada. Tras la fecha de entrega os daré un ejemplo de solución explicado con todo el detalle posible.\n",
    "\n",
    "¡Ánimo y buena suerte!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respuesta\n",
    "He estado mirando por internet como hacer este problema.\n",
    "Vamos a crear una matriz donde las filas sean los comentarios y las columnas, las palabras mas repetidas.\n",
    "En cada celda de la matriz, obtendremos las veces que esa palabra se repite.\n",
    "Esta matriz, constituirá nuestras _features_ y el _label_ será una columna de 1 y 0 donde 1 significa comentario positivo y 0 negativo.\n",
    "Para realizar esto necesitamos:\n",
    "\n",
    "* limpiar cada comentario de caracteres que no son letras\n",
    "* crear un \"Word of Bag\", donde ponemos las ocurrencias de cada palabra y el impacto (numero de ocurrencias)\n",
    "* Elegir algun clasificador que funcione bien con lenguaje natural (RandomForests,Naive-Bayes..)\n",
    "* hacer la prediccion y el posterios _score_\n",
    "\n",
    "Me he basado en este tutorial de Kaggle:\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "\n",
    "Y viendo que el tamaño de las matriz del WordOfBags puede ser enorme (unos 5.5 gigas para 1000 palabras), he tenido que limitar el numero de palabras a contar a 300 para llegar a buen puerto.\n",
    "\n",
    "## procesando el dataset.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bellinsky/anaconda3/lib/python3.5/site-packages/IPython/core/magics/extension.py:47: UserWarning: %install_ext` is deprecated, please distribute your extension as a python package.\n",
      "  \"as a python package.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed autotime.py. To use it, type:\n",
      "  %load_ext autotime\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 9.8 s\n"
     ]
    }
   ],
   "source": [
    "%install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset=pd.read_csv(\"amazon_es_reviews.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "#añadimos una columna \"positivo\" y mapeamos los comentarios a 1 o 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comentario</th>\n",
       "      <th>estrellas</th>\n",
       "      <th>positivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Para chicas es perfecto, ya que la esfera no e...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muy floja la cuerda y el anclaje es de mala ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Razonablemente bien escrito, bien ambientado, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hola! No suel o escribir muchas opiniones sobr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A simple vista m parecia una buena camara pero...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NI para pasar el rato, los personajes no tiene...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>el fabricante decia que es compatible con la d...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>el libro está en muy buenas condiciones, pero ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>buen aspecto, pero le falta fortaleza. util pa...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Explica de forma simple y sencilla los pensami...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comentario  estrellas  positivo\n",
       "0  Para chicas es perfecto, ya que la esfera no e...        4.0         1\n",
       "1  Muy floja la cuerda y el anclaje es de mala ca...        1.0         0\n",
       "2  Razonablemente bien escrito, bien ambientado, ...        3.0         0\n",
       "3  Hola! No suel o escribir muchas opiniones sobr...        5.0         1\n",
       "4  A simple vista m parecia una buena camara pero...        1.0         0\n",
       "5  NI para pasar el rato, los personajes no tiene...        1.0         0\n",
       "6  el fabricante decia que es compatible con la d...        2.0         0\n",
       "7  el libro está en muy buenas condiciones, pero ...        3.0         0\n",
       "8  buen aspecto, pero le falta fortaleza. util pa...        3.0         0\n",
       "9  Explica de forma simple y sencilla los pensami...        5.0         1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36.4 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset[\"positivo\"]=dataset.apply(lambda x: int(1) if x[\"estrellas\"]>3 else int(0), axis=1)\n",
    "\n",
    "dataset[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comentario</th>\n",
       "      <th>positivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Para chicas es perfecto, ya que la esfera no e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muy floja la cuerda y el anclaje es de mala ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Razonablemente bien escrito, bien ambientado, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hola! No suel o escribir muchas opiniones sobr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A simple vista m parecia una buena camara pero...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NI para pasar el rato, los personajes no tiene...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>el fabricante decia que es compatible con la d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>el libro está en muy buenas condiciones, pero ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>buen aspecto, pero le falta fortaleza. util pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Explica de forma simple y sencilla los pensami...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comentario  positivo\n",
       "0  Para chicas es perfecto, ya que la esfera no e...         1\n",
       "1  Muy floja la cuerda y el anclaje es de mala ca...         0\n",
       "2  Razonablemente bien escrito, bien ambientado, ...         0\n",
       "3  Hola! No suel o escribir muchas opiniones sobr...         1\n",
       "4  A simple vista m parecia una buena camara pero...         0\n",
       "5  NI para pasar el rato, los personajes no tiene...         0\n",
       "6  el fabricante decia que es compatible con la d...         0\n",
       "7  el libro está en muy buenas condiciones, pero ...         0\n",
       "8  buen aspecto, pero le falta fortaleza. util pa...         0\n",
       "9  Explica de forma simple y sencilla los pensami...         1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 842 ms\n"
     ]
    }
   ],
   "source": [
    "#creamos un nuevo dataset solo con las columnas comentario y positivo\n",
    "ds=dataset\n",
    "del ds[\"estrellas\"]\n",
    "ds[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para chicas es perfecto, ya que la esfera no es muy grande y la correa se adapta a las muñecas más finas. Un pelín gordo (para mi gusto). La carga por movimiento no dura mucho. Después de 1-2 días sin llevarlo se para.\n",
      "La funda es tal y como se muestra en las imágenes. Algo a mejorar: la tira para colgar debería ser un poco más consistente y quizas si tubiera 2 cremalleras seria más cómodo. Yo la compré para una canon g15 y el tamaño es ideal aunque si llevas la correa de la cámara para colgarla al cuello sin funda, en el momento de guardarla en la funda queda un poco justa.\n",
      "time: 62.1 ms\n"
     ]
    }
   ],
   "source": [
    "#vamos a ver que pinta tienen los comentarios, por si hay que pasarle algun parser, imprimimos algunos de ellos..\n",
    "print(ds[\"comentario\"][0])\n",
    "print(ds[\"comentario\"][40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solo tenemos carateres del tipo \",(;).\" habrá que eliminarlos para tener un WordOfBags (WOB) limpio.\n",
    "\n",
    "Tendremos que decidir que hacemos con las palabras que no tienen un significado \"sentimental\", es decir, aquellas que no conllevan juicios de valor. Para ello, existe una libreria en Python que trae listas de \"stop Words\" en cualquier idioma. Para ello, debemos importar la librearia NTLK (Python Natural Language Toolkit y acceder al metodo: stop_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 781 ms\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download()  esto solo hay que hacerlo una vez\n",
    "stopwords_sp=stopwords.words(\"spanish\")  #lista de stopwords en español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando las features desde el Bag of words\n",
    "\n",
    "Ahora que tenemos los comentarios, la pregunta es:¿cómo los convertimos en alguna representación matemática compatible con Machine Learning?\n",
    "Se podría contar el número de veces que una palabra se repite, por ejemplo:\n",
    "\n",
    "\"el raton está en la cocina\"\n",
    "\"el gato persigue al raton\"\n",
    "\n",
    "de estas dos frases, nuestro vocabulario sería:\n",
    "{ el, ratón, está, en, la, cocina,gato, persigue, al }\n",
    "\n",
    "contando las veces que aparece cada palabra en cada frase, tendriamos los siguientes ocurrencias:\n",
    "\n",
    "{2,2,1,1,1,1,1,1,1} (\"el\" aparece 2 veces, \"ratón\" otas 2 y asi..)\n",
    "\n",
    "De esta forma, podrímos quedarnos con las palabras que mas se repiten en nuestros comentarios, para ello vamos a crear los VectorizeCounters:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-ee096a89493f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# y finalmente creamos un array de features, que tiene tantas filas como comentarios y en cada columna\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# el numero de veces que aparece cada palabra de las 100 mas repetidas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrain_data_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"comentario\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bellinsky/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 824\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bellinsky/anaconda3/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34min 17s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "MAX_WORDS=300\n",
    "# inicializamos el CountVectorizer, con un maximo de MAX_WORDS palabras\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             ngram_range=(1, 2),\n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = stopwords_sp)  \n",
    "#                             max_features = MAX_WORDS) #me quedo con las MAX_WORDS palabras que mas se repiten\n",
    "\n",
    "# y finalmente creamos un array de features, que tiene tantas filas como comentarios y en cada columna\n",
    "# el numero de veces que aparece cada palabra de las 100 mas repetidas\n",
    "train_data_features = vectorizer.fit_transform(ds[\"comentario\"])\n",
    "print(type(train_data_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos a ver que palabras contiene nuestro vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print (vocab[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dist = np.sum(train_data_features.toarray(), axis=0)\n",
    "\n",
    "#vamos a imprimir cada palabra unica y el numero de ocurrencias\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print ( count, tag )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entrenando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(train_data_features))\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos una matriz de 702446 filas y 100 columnas. He tenido que restringir el numero de palabras a 100 puesto que con 5000 se va de memoria. \n",
    "Nos queda unicamente concatenar a esta matriz, el vector de positivo/negativo y llamar a algun clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#como el train_data_features es una matriz disperas, usamos los metodos de la libreria scipy.sparse para concatenar ambas matrices\n",
    "vector_positivo=csr_matrix(ds[\"positivo\"]).T\n",
    "\n",
    "sp_dataset=sp.hstack([train_data_features,vector_positivo]).toarray()\n",
    "#comprobamos que lo hemos hecho bien, observando la forma de cada matriz\n",
    "print(vector_positivo.shape)\n",
    "print(train_data_features.shape)\n",
    "print(sp_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datos_spliteados = train_test_split(sp_dataset,\n",
    "                                    train_size=0.8, # 80% training\n",
    "                                    test_size=0.2   # 20% testing\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#separamos los datos en train y test\n",
    "train=datos_spliteados[0]\n",
    "test=datos_spliteados[1]\n",
    "\n",
    "train_label=train[:,-1]\n",
    "test_label=test[:,-1]\n",
    "\n",
    "train_features= np.delete(train, MAX_WORDS, axis=1)\n",
    "test_features=np.delete(test,MAX_WORDS,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# probamos ahora con un logistic regressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(test_features, test_label)\n",
    "print(\"bien clasificados en test: %.16f\" % lr.score(X=test_features,y=test_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-820ec8ae0711>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bien clasificados en test: %.16f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bellinsky/anaconda3/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bellinsky/anaconda3/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/bellinsky/anaconda3/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mjointi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_prior_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mn_ij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigma_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n\u001b[0m\u001b[0;32m    430\u001b[0m                                  (self.sigma_[i, :]), 1)\n\u001b[0;32m    431\u001b[0m             \u001b[0mjoint_log_likelihood\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjointi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_ij\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "# probamos ahora con un Naives-Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(test_features, test_label)\n",
    "print(\"bien clasificados en test: %.16f\" % gnb.score(X=test_features,y=test_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Probamos con un RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "forest = forest.fit( train_features, train_label )\n",
    "print(\"Bien clasificados en test: %.16f\" % forest.score(X=test_features,y=test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
