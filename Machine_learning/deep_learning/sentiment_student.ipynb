{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment: analyzing movie reviews with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<img src=\"img/cinemaReviews.png\" style=\"width:600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "In this assignment we will analyze the sentiment, positive or negative, expressed in a set of movie reviews IMDB. To do so we will make use of word embeddings and recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n",
    "\n",
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>You will need to solve a question by writing your own code or answer in the cell immediately below, or in a different file as instructed.</td></tr>\n",
    " <tr><td><img src=\"img/exclamation.png\" style=\"width:80px;height:80px;\"></td><td>This is a hint or useful observation that can help you solve this assignment. You are not expected to write any solution, but you should pay attention to them to understand the assignment.</td></tr>\n",
    " <tr><td><img src=\"img/pro.png\" style=\"width:80px;height:80px;\"></td><td>This is an advanced and voluntary exercise that can help you gain a deeper knowledge into the topic. Good luck!</td></tr>\n",
    "</table>\n",
    "\n",
    "During the assigment you will make use of several Python packages that might not be installed in your machine. If that is the case, you can install new Python packages with\n",
    "\n",
    "    conda install PACKAGENAME\n",
    "    \n",
    "if you are using Python Anaconda. Else you should use\n",
    "\n",
    "    pip install PACKAGENAME\n",
    "\n",
    "You will need the following packages for this particular assignment. Make sure they are available before proceeding:\n",
    "\n",
    "* **numpy**\n",
    "* **keras**\n",
    "* **matplotlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will embed any plots into the notebook instead of generating a new window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells. \n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will make use of the <a href=http://keras.io/>keras</a> Deep Learning library for Python. This library allows building several kinds of shallow and deep networks, following either a sequential or a graph architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of a part of the IMDB database on movie reviews. IMDB rates movies with a score ranging 0-10, but for simplicity we will consider a dataset of good and bad reviews, where a review has been considered bad with a score smaller than 4, and good if it features a score larger than 7. The data is available under the *data* folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Load the data into two variables, a list **text** with each of the movie reviews and a list **y** of the class labels.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bellinsky/Documents/datahack/algoritmos_avanzados/deeplearning/lab2_sentiment'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset=pd.read_csv('./data/data.csv',sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience in what follows we will also split the data into a training and test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Split the list of texts into **texts_train** and **texts_test** lists, keeping 25% of the texts for test. Split in the same way the labels, obtaining lists **y_train** and **y_test**.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datos_spliteados = train_test_split(dataset,\n",
    "                                    train_size=0.75, # 80% training\n",
    "                                    test_size=0.25   # 20% testing\n",
    "                                   )\n",
    "texts_train_df=datos_spliteados[0]\n",
    "texts_test_df=datos_spliteados[1]\n",
    "\n",
    "texts_train=texts_train_df['text'].tolist()\n",
    "texts_test=texts_test_df['text'].tolist()\n",
    "\n",
    "y_train=np.array(texts_train_df['sentiment'].tolist())\n",
    "y_test=np.array(texts_test_df['sentiment'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't introduce text directly into the network, so we will have to tranform it to a vector representation. To do so, we will first **tokenize** the text into words (or tokens), and assign a unique identifier to each word found in the text. Doing this will allow us to perform the encoding. We can do this easily by making use of the **Tokenizer** class in keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tokenizer offers convenient methods to split texts down to tokens. At construction time we need to supply the Tokenizer the maximum number of different words we are willing to represent. If out texts have greater word variety than this number, the least frequent words will be discarded. We will choose a number large enough for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxwords = 1000\n",
    "tokenizer = Tokenizer(nb_words = maxwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to **fit** the Tokenizer to the training texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Find in the keras documentation the appropriate Tokenizer method to fit the tokenizer on a list of text, then use it to fit it on the training data.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, the following should show the number of times the tokenizer has found each word in the input texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rayburn': 1,\n",
       " 'assistant': 15,\n",
       " 'neeson': 3,\n",
       " 'retrouvé': 1,\n",
       " 'guy': 239,\n",
       " 'strait': 2,\n",
       " 'diversity': 6,\n",
       " 'depraved': 2,\n",
       " 'minutes\\x85': 1,\n",
       " 'sens': 1,\n",
       " 'ugc': 1,\n",
       " 'says': 81,\n",
       " 'sleaziest': 1,\n",
       " 'well': 842,\n",
       " 'thrilled': 4,\n",
       " 'pejorative': 1,\n",
       " 'blasters': 1,\n",
       " 'trendier': 1,\n",
       " \"p'tite\": 1,\n",
       " \"'nam\": 2,\n",
       " 'rely': 3,\n",
       " 'justice': 37,\n",
       " 'demonstrate': 2,\n",
       " 'dodge': 6,\n",
       " 'framing': 2,\n",
       " 'consideration': 4,\n",
       " 'superhuman': 2,\n",
       " '1984': 5,\n",
       " 'suave': 4,\n",
       " 'pen': 2,\n",
       " 'awhile': 8,\n",
       " 'fussy': 3,\n",
       " 'honorary': 1,\n",
       " \"kudos'\": 1,\n",
       " 'colorado': 1,\n",
       " 'bloodthirsty': 2,\n",
       " 'forward': 55,\n",
       " \"something's\": 1,\n",
       " 'oversight': 4,\n",
       " 'miserable': 9,\n",
       " 'tutee': 1,\n",
       " 'mike': 23,\n",
       " 'keanu': 4,\n",
       " \"cliché's\": 3,\n",
       " 'away': 246,\n",
       " 'fiendish': 1,\n",
       " 'allergic': 1,\n",
       " 'manifested': 1,\n",
       " 'densely': 1,\n",
       " 'golan': 1,\n",
       " 'reductionism': 1,\n",
       " 'multiculturalism': 1,\n",
       " 'hour': 95,\n",
       " 'titillate': 1,\n",
       " 'calamari': 1,\n",
       " 'dreadfully': 2,\n",
       " 'raped': 12,\n",
       " 'infiltration': 1,\n",
       " 'hide': 21,\n",
       " 'transient': 1,\n",
       " 'telescope': 1,\n",
       " 'whorehouse': 1,\n",
       " 'venturing': 2,\n",
       " 'bastards': 2,\n",
       " 'qt': 1,\n",
       " 'watches': 9,\n",
       " 'stomach': 8,\n",
       " 'chin': 2,\n",
       " 'bure': 1,\n",
       " 'plotting': 3,\n",
       " \"'silence'\": 1,\n",
       " 'close': 93,\n",
       " 'blvd': 1,\n",
       " 'largest': 2,\n",
       " 'symbolizes': 1,\n",
       " 'pace': 38,\n",
       " 'executes': 3,\n",
       " 'smashed': 1,\n",
       " 'abuser': 1,\n",
       " 'excused': 2,\n",
       " 'cahill': 1,\n",
       " 'lawn': 4,\n",
       " 'manhandling': 1,\n",
       " 'fortuitously': 1,\n",
       " 'kantrowitz': 2,\n",
       " 'comedic': 30,\n",
       " 'writing': 91,\n",
       " 'cab': 2,\n",
       " 'vijay': 1,\n",
       " 'behaviour': 4,\n",
       " 'repented': 1,\n",
       " 'hinting': 3,\n",
       " 'reasoned': 1,\n",
       " 'done': 226,\n",
       " 'yeardley': 1,\n",
       " 'remarque': 1,\n",
       " 'saying': 78,\n",
       " 'finger': 9,\n",
       " 'presences': 1,\n",
       " 'surpasses': 2,\n",
       " 'spines': 2,\n",
       " 'restriction': 1,\n",
       " 'morphin': 2,\n",
       " 'mulligan': 1,\n",
       " 'complemented': 2,\n",
       " 'remain': 9,\n",
       " 'wisely': 8,\n",
       " 'keyboards': 2,\n",
       " 'elementary': 3,\n",
       " 'informants': 1,\n",
       " 'mariangela': 1,\n",
       " 'howard': 22,\n",
       " 'client': 5,\n",
       " 'critique': 7,\n",
       " 'leukemia': 1,\n",
       " 'transpired': 1,\n",
       " 'questioned': 3,\n",
       " 'avidly': 1,\n",
       " 'overriding': 1,\n",
       " 'explosions': 10,\n",
       " 'joyride': 1,\n",
       " 'immediate': 8,\n",
       " 'saura': 2,\n",
       " 'chafes': 1,\n",
       " 'kinda': 13,\n",
       " 'kuttram': 1,\n",
       " 'calamitous': 1,\n",
       " '2004': 8,\n",
       " 'temps': 3,\n",
       " 'tank': 9,\n",
       " 'misunderstood': 10,\n",
       " 'importantly': 7,\n",
       " 'land': 26,\n",
       " 'formula': 25,\n",
       " 'cp': 1,\n",
       " 'absurdly': 4,\n",
       " 'aftertaste': 1,\n",
       " 'attracts': 1,\n",
       " 'twists': 26,\n",
       " 'ramifications': 1,\n",
       " 'chilling': 9,\n",
       " 'repay': 2,\n",
       " 'plumbing': 1,\n",
       " 'sighing': 1,\n",
       " 'uncommon': 2,\n",
       " 'streak': 2,\n",
       " \"reed's\": 1,\n",
       " 'forme': 1,\n",
       " 'cans': 1,\n",
       " 'giddily': 1,\n",
       " 'soda': 2,\n",
       " 'clicks': 1,\n",
       " 'feeling': 90,\n",
       " 'traveller': 1,\n",
       " 'tame': 9,\n",
       " \"jesus'\": 1,\n",
       " 'dual': 2,\n",
       " 'raymond': 11,\n",
       " 'twosome': 1,\n",
       " 'awfulness': 3,\n",
       " 'pool': 18,\n",
       " 'binks': 1,\n",
       " 'preacher': 3,\n",
       " 'pinnacle': 1,\n",
       " 'cinemaniaks': 1,\n",
       " 'remastering': 1,\n",
       " 'sepia': 2,\n",
       " 'misconceptions': 1,\n",
       " 'enaction': 1,\n",
       " 'screens': 5,\n",
       " \"kitamura's\": 1,\n",
       " 'already': 94,\n",
       " 'biopic': 5,\n",
       " 'buckle': 1,\n",
       " 'lott': 1,\n",
       " 'adheres': 1,\n",
       " 'eighties': 7,\n",
       " 'oooh': 1,\n",
       " 'comiccon': 1,\n",
       " 'yankee': 2,\n",
       " 'psychopaths': 1,\n",
       " 'tony': 33,\n",
       " 'daylight': 3,\n",
       " 'dependency': 3,\n",
       " 'adams': 8,\n",
       " 'fatso': 1,\n",
       " 'crackpot': 1,\n",
       " 'brian': 23,\n",
       " 'streisand': 3,\n",
       " 'care': 102,\n",
       " 'patriots': 1,\n",
       " 'steam': 7,\n",
       " 'elaine': 1,\n",
       " 'lemay': 2,\n",
       " 'fish': 10,\n",
       " 'females': 6,\n",
       " 'attendants': 1,\n",
       " 'restoring': 1,\n",
       " \"henry's\": 2,\n",
       " 'thin': 26,\n",
       " 'sentenced': 6,\n",
       " 'hardness': 1,\n",
       " 'cube': 17,\n",
       " '1951': 9,\n",
       " 'smooth': 8,\n",
       " 'defined': 2,\n",
       " 'sacrifices': 5,\n",
       " 'motorbike': 1,\n",
       " 'qestions': 1,\n",
       " 'alexis': 1,\n",
       " 'lies': 18,\n",
       " 'housekeeper': 1,\n",
       " 'one': 2004,\n",
       " 'available': 48,\n",
       " 'loveless': 1,\n",
       " 'dismisses': 1,\n",
       " 'massachusetts': 1,\n",
       " 'gruesomely': 2,\n",
       " 'philosophy': 7,\n",
       " \"money's\": 1,\n",
       " 'eyebrow': 1,\n",
       " 'several': 125,\n",
       " 'inspirational': 5,\n",
       " \"cameron's\": 1,\n",
       " \"president'\": 1,\n",
       " 'amphibious': 1,\n",
       " 'jodelle': 1,\n",
       " 'hers': 6,\n",
       " 'arched': 1,\n",
       " 'recollection': 1,\n",
       " 'era': 58,\n",
       " 'fathers': 6,\n",
       " 'stray': 3,\n",
       " 'interwoven': 1,\n",
       " 'abstractions': 1,\n",
       " \"paul's\": 3,\n",
       " 'to': 10447,\n",
       " 'emigrates': 1,\n",
       " 'noble': 9,\n",
       " 'supremacists': 1,\n",
       " 'strategy': 4,\n",
       " 'blamed': 5,\n",
       " 'scrap': 1,\n",
       " 'honorable': 4,\n",
       " 'undersea': 1,\n",
       " 'ask': 52,\n",
       " 'copper': 4,\n",
       " 'sticking': 3,\n",
       " 'closing': 10,\n",
       " 'achingly': 1,\n",
       " 'mandark': 1,\n",
       " 'pondering': 1,\n",
       " 'victim': 30,\n",
       " 'consensus': 8,\n",
       " 'underdeveloped': 6,\n",
       " 'kip': 4,\n",
       " 'self': 76,\n",
       " 'alarm': 3,\n",
       " 'pixar': 4,\n",
       " 'vested': 1,\n",
       " 'observations': 3,\n",
       " 'iron': 10,\n",
       " 'moronie': 4,\n",
       " 'argues': 1,\n",
       " 'limits': 9,\n",
       " 'below': 16,\n",
       " 'survivors': 8,\n",
       " 'bare': 8,\n",
       " 'porno': 6,\n",
       " 'contamination': 1,\n",
       " 'pattern': 5,\n",
       " 'cut': 86,\n",
       " 'familiarity': 1,\n",
       " \"'parkinson'\": 1,\n",
       " 'dating': 6,\n",
       " 'goebbels': 1,\n",
       " 'redeem': 5,\n",
       " 'leer': 1,\n",
       " 'transcend': 3,\n",
       " 'luxuriant': 1,\n",
       " 'warehouses': 1,\n",
       " 'solved': 3,\n",
       " 'thats': 27,\n",
       " 'passable': 1,\n",
       " 'disbelieve': 1,\n",
       " 'genuine': 13,\n",
       " 'littered': 1,\n",
       " 'unions': 4,\n",
       " 'deodato': 2,\n",
       " 'planning': 10,\n",
       " 'hack': 6,\n",
       " \"thor's\": 1,\n",
       " 'stuttgart': 1,\n",
       " 'patching': 1,\n",
       " 'brights': 1,\n",
       " 'hanks': 7,\n",
       " 'researched': 4,\n",
       " 'vacuum': 2,\n",
       " 'eve': 8,\n",
       " 'acted': 53,\n",
       " 'brashear': 1,\n",
       " 'instilled': 1,\n",
       " 'choices': 9,\n",
       " \"'toots'\": 1,\n",
       " 'diversion': 3,\n",
       " \"'moebius'\": 1,\n",
       " 'treating': 1,\n",
       " 'percolating': 1,\n",
       " 'excuse': 35,\n",
       " 'nick': 33,\n",
       " 'tossing': 3,\n",
       " 'bland': 18,\n",
       " \"classic'\": 1,\n",
       " 'ferzan': 1,\n",
       " \"games'\": 2,\n",
       " 'has': 1350,\n",
       " 'guiness': 2,\n",
       " 'arliss': 1,\n",
       " 'chamber': 1,\n",
       " 'smith': 62,\n",
       " 'morons': 5,\n",
       " 'dampened': 1,\n",
       " 'accompanying': 1,\n",
       " 'additionally': 5,\n",
       " 'had': 803,\n",
       " 'snippets': 2,\n",
       " 'usually': 78,\n",
       " 'neil': 12,\n",
       " 'proceedings': 7,\n",
       " 'does\\x85': 1,\n",
       " 'cartoonist': 1,\n",
       " 'campaign': 7,\n",
       " 'staden': 6,\n",
       " 'sharers': 1,\n",
       " 'pro': 13,\n",
       " 'madly': 1,\n",
       " 'opined': 1,\n",
       " 'countess': 4,\n",
       " 'simpler': 2,\n",
       " 'spectacularly': 2,\n",
       " 'booze': 3,\n",
       " 'akosua': 1,\n",
       " 'heartily': 1,\n",
       " 'liam': 2,\n",
       " 'rhythm': 5,\n",
       " 're': 59,\n",
       " 'facial': 11,\n",
       " 'phrases': 2,\n",
       " 'johnnie': 2,\n",
       " 'melts': 2,\n",
       " 'measure': 10,\n",
       " 'ince': 1,\n",
       " 'materialized': 1,\n",
       " 'label': 4,\n",
       " 'complained': 6,\n",
       " 'fetch': 1,\n",
       " 'scrutiny': 1,\n",
       " 'matchstick': 1,\n",
       " 'admitting': 2,\n",
       " 'researcher': 2,\n",
       " 'bully': 1,\n",
       " '“i’d': 1,\n",
       " 'labelled': 1,\n",
       " 'julie': 16,\n",
       " 'beals': 2,\n",
       " 'labour': 2,\n",
       " 'cashes': 1,\n",
       " 'blasphemy': 1,\n",
       " 'irascible': 1,\n",
       " 'liné': 1,\n",
       " 'mistress': 5,\n",
       " 'chronological': 1,\n",
       " 'enters': 11,\n",
       " 'upping': 1,\n",
       " 'womb': 1,\n",
       " 'collaborator': 4,\n",
       " 'police': 99,\n",
       " \"halloween's\": 1,\n",
       " 'sinister': 9,\n",
       " 'captivate': 2,\n",
       " 'doorstep': 1,\n",
       " 'custom': 1,\n",
       " 'pinned': 2,\n",
       " 'patsy': 2,\n",
       " 'mongering': 1,\n",
       " 'yelnats': 3,\n",
       " 'antichrist': 1,\n",
       " 'creativeness': 1,\n",
       " 'recruits': 1,\n",
       " 'periodicals': 1,\n",
       " 'correctly': 8,\n",
       " 'beginnings': 3,\n",
       " 'thalmus': 1,\n",
       " 'jerkwad': 1,\n",
       " 'underpinned': 1,\n",
       " 'magnet': 1,\n",
       " 'relic': 1,\n",
       " 'discrepancies': 2,\n",
       " '1999': 7,\n",
       " 'cliched': 5,\n",
       " 'navuoo': 1,\n",
       " 'assistants': 2,\n",
       " 'landfall': 1,\n",
       " 'notting': 1,\n",
       " 'haphazard': 1,\n",
       " 'thumbs': 12,\n",
       " 'taking': 74,\n",
       " 'fulfilled': 4,\n",
       " 'board': 23,\n",
       " 'switchblade': 2,\n",
       " 'camille': 1,\n",
       " 'poundage': 1,\n",
       " 'upside': 5,\n",
       " 'clarity': 4,\n",
       " 'burman': 1,\n",
       " 'rajinikanth': 3,\n",
       " 'dis': 3,\n",
       " 'sharp': 14,\n",
       " 'affirmatively': 1,\n",
       " 'hebrew': 2,\n",
       " 'lassie': 1,\n",
       " 'bollywood': 9,\n",
       " 'atlantian': 4,\n",
       " 'performing': 9,\n",
       " 'period': 64,\n",
       " 'cajoled': 1,\n",
       " 'tanny': 2,\n",
       " 'cliffs': 1,\n",
       " 'chapman': 1,\n",
       " 'movie': 3221,\n",
       " 'pisses': 1,\n",
       " 'mutant': 5,\n",
       " 'nisha': 2,\n",
       " 'rudyard': 1,\n",
       " 'rhyme': 3,\n",
       " 'dickey': 1,\n",
       " 'doubles': 3,\n",
       " 'historic': 2,\n",
       " 'cebuano': 1,\n",
       " 'permitted': 1,\n",
       " 'tempo': 2,\n",
       " 'annoy': 3,\n",
       " 'maintained': 3,\n",
       " 'shrunk': 2,\n",
       " 'kwame': 2,\n",
       " 'large': 42,\n",
       " 'harbour': 1,\n",
       " 'popinjay': 1,\n",
       " 'abusive': 6,\n",
       " 'regency': 1,\n",
       " 'oater': 2,\n",
       " 'bachelors': 1,\n",
       " 'bond': 26,\n",
       " 'reminding': 2,\n",
       " 'darnedest': 1,\n",
       " 'who': 1587,\n",
       " 'ally': 3,\n",
       " 'unfocused': 1,\n",
       " 'witnessed': 7,\n",
       " 'séance': 2,\n",
       " 'center': 25,\n",
       " 'trespassed': 1,\n",
       " 'resses': 1,\n",
       " 'stepmother': 2,\n",
       " \"imamura's\": 1,\n",
       " 'paces': 2,\n",
       " 'pg': 12,\n",
       " 'throat': 10,\n",
       " 'hundreds': 17,\n",
       " 'photograph': 1,\n",
       " \"cowboy'\": 7,\n",
       " 'raises': 8,\n",
       " 'deconstructing': 1,\n",
       " 'sci': 41,\n",
       " 'moonstruck': 3,\n",
       " 'grieving': 2,\n",
       " 'skits': 5,\n",
       " 'shebang': 1,\n",
       " \"york's\": 1,\n",
       " 'polo': 3,\n",
       " 'fatigue': 1,\n",
       " 'swimmingly': 1,\n",
       " 'illegally': 2,\n",
       " 'treasures': 2,\n",
       " 'scam': 1,\n",
       " \"mendel's\": 1,\n",
       " 'embellishments': 4,\n",
       " 'hunger': 8,\n",
       " 'wwii': 19,\n",
       " 'grisly': 1,\n",
       " 'starting': 19,\n",
       " 'suffocates': 1,\n",
       " 'misogyny': 2,\n",
       " 'tamer': 1,\n",
       " 'sandrich': 1,\n",
       " 'divorces': 1,\n",
       " 'donald': 11,\n",
       " 'say\\x85': 1,\n",
       " 'stacking': 3,\n",
       " 'commentators': 3,\n",
       " 'greenland': 1,\n",
       " 'gadgets': 3,\n",
       " 'klusak': 1,\n",
       " 'surroundings': 1,\n",
       " 'cigar': 1,\n",
       " 'beer': 11,\n",
       " 'ugliest': 2,\n",
       " 'choco': 1,\n",
       " 'encountered': 3,\n",
       " 'smash': 9,\n",
       " 'fog': 3,\n",
       " 'baddie': 5,\n",
       " 'seasoned': 5,\n",
       " 'suffers': 14,\n",
       " 'towards': 40,\n",
       " 'methodist': 1,\n",
       " 'controls': 7,\n",
       " 'verse': 1,\n",
       " 'bloom': 2,\n",
       " 'cost': 16,\n",
       " 'gawd': 4,\n",
       " 'slaves': 2,\n",
       " 'attachments': 3,\n",
       " 'expression': 13,\n",
       " 'shirt': 11,\n",
       " 'retell': 1,\n",
       " 'lobotomized': 1,\n",
       " 'greg': 8,\n",
       " 'definitive': 7,\n",
       " 'moran': 2,\n",
       " 'refrigerator': 1,\n",
       " \"linklater's\": 1,\n",
       " 'klaw': 2,\n",
       " 'freeview': 1,\n",
       " 'scrapping': 1,\n",
       " 'englishman': 1,\n",
       " 'dashing': 3,\n",
       " 'garbo': 3,\n",
       " 'zone': 8,\n",
       " 'apartment': 27,\n",
       " \"your'e\": 1,\n",
       " 'screweyes': 3,\n",
       " 'through': 355,\n",
       " 'chritmas': 1,\n",
       " \"friggin'\": 1,\n",
       " 'marcy': 8,\n",
       " 'possessive': 1,\n",
       " 'ring': 28,\n",
       " 'basis': 13,\n",
       " 'pics': 1,\n",
       " 'earn': 5,\n",
       " 'runic': 2,\n",
       " 'jenner': 1,\n",
       " 'halted': 1,\n",
       " 'morneau': 1,\n",
       " 'shaolin': 4,\n",
       " 'compromised': 4,\n",
       " 'draws': 13,\n",
       " 'drifting': 2,\n",
       " 'mia': 6,\n",
       " 'tree': 15,\n",
       " 'meeker': 4,\n",
       " 'attacking': 6,\n",
       " 'polygamy': 1,\n",
       " 'trapping': 1,\n",
       " 'exploit': 7,\n",
       " 'financing': 1,\n",
       " 'regarded': 6,\n",
       " \"mcdermott's\": 1,\n",
       " 'convey': 15,\n",
       " 'chinese': 29,\n",
       " 'audio': 12,\n",
       " 'fx': 7,\n",
       " 'iliada': 1,\n",
       " 'blackie': 4,\n",
       " 'ithaca': 1,\n",
       " 'drusilla': 1,\n",
       " 'saturn': 4,\n",
       " \"winner's\": 1,\n",
       " 'japs': 1,\n",
       " 'orry': 1,\n",
       " \"emmy's\": 1,\n",
       " 'brinke': 1,\n",
       " \"1956's\": 1,\n",
       " 'centenary': 1,\n",
       " 'continue': 22,\n",
       " 'likeability': 1,\n",
       " 'scares': 11,\n",
       " 'rent': 57,\n",
       " 'unexpectedly': 3,\n",
       " 'fits': 22,\n",
       " 'mizz': 1,\n",
       " 'established': 13,\n",
       " 'assistance': 2,\n",
       " 'hollow': 9,\n",
       " 'finlayson': 1,\n",
       " 'lamp': 2,\n",
       " 'enraged': 1,\n",
       " 'anticipated': 4,\n",
       " 'blanca': 1,\n",
       " 'padayappa': 1,\n",
       " 'swedish': 7,\n",
       " 'bratislav': 1,\n",
       " 'commissioned': 4,\n",
       " 'richie': 2,\n",
       " 'stages': 7,\n",
       " 'bleed': 1,\n",
       " \"rapists's\": 1,\n",
       " 'disagreement': 1,\n",
       " 'risen': 2,\n",
       " 'lot': 302,\n",
       " 'kung': 26,\n",
       " 'great': 673,\n",
       " 'aoki': 1,\n",
       " 'shipyard': 1,\n",
       " 'bislane': 3,\n",
       " 'eaters': 1,\n",
       " 'gunslinger': 1,\n",
       " 'hanging': 17,\n",
       " 'curley': 1,\n",
       " '1964': 8,\n",
       " 'coal': 7,\n",
       " \"victor's\": 1,\n",
       " 'perpetual': 2,\n",
       " 'curator': 1,\n",
       " 'palmer': 2,\n",
       " 'tepper': 1,\n",
       " 'flashier': 1,\n",
       " 'macdonald': 2,\n",
       " 'tyson': 1,\n",
       " 'reserved': 6,\n",
       " 'elaborate': 12,\n",
       " 'anticlimactic': 1,\n",
       " 'detained': 1,\n",
       " 'understanding': 20,\n",
       " 'flickers': 2,\n",
       " 'stood': 10,\n",
       " 'dispensing': 1,\n",
       " 'faux': 5,\n",
       " 'jin': 3,\n",
       " 'lice': 1,\n",
       " 'ropes': 1,\n",
       " 'irrational': 1,\n",
       " \"herman's\": 1,\n",
       " 'intellectual': 11,\n",
       " 'humiliating': 2,\n",
       " 'vamp': 3,\n",
       " 'murderer': 12,\n",
       " 'misplaced': 2,\n",
       " 'cod': 3,\n",
       " 'astral': 4,\n",
       " 'curled': 1,\n",
       " 'solitary': 1,\n",
       " 'reposed': 1,\n",
       " 'manhatten': 1,\n",
       " 'rights': 17,\n",
       " 'twentieth': 1,\n",
       " 'frights': 1,\n",
       " 'grille': 2,\n",
       " 'spaceflight': 1,\n",
       " 'abrupt': 3,\n",
       " \"p'z\": 1,\n",
       " \"twin's\": 1,\n",
       " 'airlock': 1,\n",
       " 'hadley': 9,\n",
       " 'bearded': 2,\n",
       " 'foran': 2,\n",
       " 'assumes': 6,\n",
       " 'tale\\x97namely': 1,\n",
       " 'disscusion': 1,\n",
       " 'rivalries': 1,\n",
       " 'pergado': 1,\n",
       " 'clear': 63,\n",
       " 'tapping': 3,\n",
       " 'impact': 27,\n",
       " 'moroder': 2,\n",
       " 'jasmin': 1,\n",
       " 'lim': 1,\n",
       " 'define': 7,\n",
       " \"collector's\": 2,\n",
       " \"nance's\": 1,\n",
       " 'sanitized': 1,\n",
       " 'vulgarity': 1,\n",
       " 'downhill': 4,\n",
       " 'stunning': 31,\n",
       " 'drooling': 1,\n",
       " 'buff': 4,\n",
       " '1939': 5,\n",
       " 'hush': 2,\n",
       " 'nephews': 1,\n",
       " 'infomercial': 1,\n",
       " \"frickin'\": 1,\n",
       " 'seymour': 7,\n",
       " 'bonanza': 16,\n",
       " 'eraser': 1,\n",
       " 'assembling': 1,\n",
       " 'mastrantonio': 1,\n",
       " 'pain': 33,\n",
       " 'excels': 4,\n",
       " 'peninsula': 1,\n",
       " 'sources': 4,\n",
       " 'looks': 204,\n",
       " \"caeser's\": 1,\n",
       " 'corporation': 5,\n",
       " 'darned': 2,\n",
       " 'waaaaay': 1,\n",
       " 'beggars': 2,\n",
       " 'tiff': 2,\n",
       " 'abuses': 2,\n",
       " 'fright': 4,\n",
       " 'peavey': 1,\n",
       " 'thunderbirds': 4,\n",
       " 'detailing': 3,\n",
       " 'representing': 3,\n",
       " 'couer': 1,\n",
       " 'despised': 2,\n",
       " 'got': 247,\n",
       " 'themed': 2,\n",
       " 'stock': 20,\n",
       " 'sappiness': 2,\n",
       " \"'penis'\": 1,\n",
       " 'fourteen': 3,\n",
       " 'plundering': 1,\n",
       " 'hands': 48,\n",
       " 'swoosh': 1,\n",
       " 'apprised': 1,\n",
       " 'incinerator': 1,\n",
       " 'belzer': 1,\n",
       " 'fired': 18,\n",
       " 'lightner': 2,\n",
       " 'emancipated': 1,\n",
       " 'imprint': 1,\n",
       " 'jokingly': 1,\n",
       " 'weirdos': 3,\n",
       " 'clise': 1,\n",
       " 'lighten': 4,\n",
       " 'scenario': 22,\n",
       " 'setup': 6,\n",
       " \"ollie'\": 1,\n",
       " 'washington': 10,\n",
       " 'surly': 2,\n",
       " 'opportunities': 2,\n",
       " 'lifetimes': 1,\n",
       " 'spiritually': 2,\n",
       " 'panaghoy': 1,\n",
       " 'fred': 22,\n",
       " 'repetitive': 14,\n",
       " 'unflattering': 1,\n",
       " 'fail': 32,\n",
       " 'socialite': 1,\n",
       " 'psychical': 4,\n",
       " 'kodokoo': 1,\n",
       " 'remaining': 10,\n",
       " 'expand': 5,\n",
       " \"akin's\": 1,\n",
       " 'resurrected': 4,\n",
       " 'operating': 2,\n",
       " 'mcgovernisms': 1,\n",
       " 'sanchez': 3,\n",
       " 'advised': 3,\n",
       " 'frost': 3,\n",
       " 'i’m': 1,\n",
       " 'chrissy': 2,\n",
       " 'predicts': 1,\n",
       " 'wally': 1,\n",
       " 'lawrence': 5,\n",
       " 'executive': 8,\n",
       " 'falk': 14,\n",
       " 'adjacent': 3,\n",
       " 'dries': 1,\n",
       " '14': 14,\n",
       " 'artwork': 7,\n",
       " \"miike's\": 6,\n",
       " \"vancouver's\": 1,\n",
       " 'yield': 1,\n",
       " 'mixing': 5,\n",
       " 'put': 174,\n",
       " 'rousset': 1,\n",
       " 'dissolve': 2,\n",
       " 'jefferson': 4,\n",
       " 'forty': 6,\n",
       " 'amazon': 8,\n",
       " 'mayor': 5,\n",
       " 'hallarious': 1,\n",
       " 'hissing': 1,\n",
       " 'do': 654,\n",
       " 'brennen': 2,\n",
       " 'spice': 1,\n",
       " 'cannibals': 3,\n",
       " 'indeed': 57,\n",
       " 'pernicious': 1,\n",
       " 'von': 9,\n",
       " 'notables': 1,\n",
       " 'scarefests': 1,\n",
       " 'lollabrigida': 1,\n",
       " 'consciously': 1,\n",
       " 'daggers': 1,\n",
       " 'brigade': 3,\n",
       " 'crouching': 1,\n",
       " 'hurry': 2,\n",
       " 'fade': 6,\n",
       " 'bourgeois': 2,\n",
       " 'jewelry': 5,\n",
       " 'cagney': 3,\n",
       " 'nowhere': 28,\n",
       " '18th': 3,\n",
       " 'infancy': 1,\n",
       " 'clumsily': 2,\n",
       " 'substantiates': 1,\n",
       " 'enjoys': 9,\n",
       " 'fifteenth': 2,\n",
       " 'crystallizes': 1,\n",
       " 'albiet': 1,\n",
       " 'samberg': 1,\n",
       " 'looser': 2,\n",
       " 'optimism': 2,\n",
       " 'sympathetic': 12,\n",
       " \"hitchcock's\": 3,\n",
       " 'hans': 3,\n",
       " 'throughout': 117,\n",
       " 'swashbuckling': 1,\n",
       " 'journal': 3,\n",
       " 'wayland': 1,\n",
       " 'bops': 4,\n",
       " 'schtupp': 1,\n",
       " 'astonish': 1,\n",
       " 'somebody': 20,\n",
       " \"modesty's\": 2,\n",
       " 'concentration': 1,\n",
       " 'plotline': 1,\n",
       " 'radicals': 1,\n",
       " 'walnuts': 1,\n",
       " 'fire': 31,\n",
       " 'chintzy': 1,\n",
       " 'comet': 1,\n",
       " 'sisley': 1,\n",
       " 'chains': 3,\n",
       " 'surgeon': 1,\n",
       " 'resist': 4,\n",
       " \"'romantic\": 1,\n",
       " 'improvised': 5,\n",
       " 'dichotomy': 1,\n",
       " 'kristy': 1,\n",
       " 'askey': 2,\n",
       " 'destroyer': 2,\n",
       " 'incompatibility': 1,\n",
       " 'administration': 4,\n",
       " 'batter': 2,\n",
       " 'co': 45,\n",
       " 'fiery': 3,\n",
       " 'fletcher': 1,\n",
       " 'midori': 1,\n",
       " 'layered': 1,\n",
       " 'downside': 2,\n",
       " 'imitators': 1,\n",
       " 'empress': 5,\n",
       " 'ulrich': 2,\n",
       " 'your': 403,\n",
       " 'laundromat': 1,\n",
       " 'mirroring': 4,\n",
       " 'gopal': 4,\n",
       " 'confused': 29,\n",
       " 'belmore': 1,\n",
       " 'visual': 38,\n",
       " 'nurturing': 1,\n",
       " 'heywood': 1,\n",
       " 'pupils': 1,\n",
       " 'searched': 3,\n",
       " 'vincent': 18,\n",
       " 'maltese': 4,\n",
       " 'factors': 3,\n",
       " 'comensurate': 1,\n",
       " \"crouse's\": 2,\n",
       " 'commercial': 16,\n",
       " 'mercer': 3,\n",
       " \"forsythe's\": 1,\n",
       " 'powerhouse': 1,\n",
       " 'horrified': 4,\n",
       " 'mittel': 1,\n",
       " 'disorienting': 1,\n",
       " 'burnett': 1,\n",
       " 'stimuli': 1,\n",
       " 'framed': 5,\n",
       " '1930': 5,\n",
       " 'italy': 18,\n",
       " 'downgrade': 1,\n",
       " 'photography': 39,\n",
       " 'colleague': 4,\n",
       " 'nerves': 3,\n",
       " 'chicago': 6,\n",
       " 'mann': 12,\n",
       " '1960': 1,\n",
       " 'gynoid': 2,\n",
       " 'compliments': 1,\n",
       " 'overall': 98,\n",
       " 'accessible': 8,\n",
       " 'middling': 1,\n",
       " \"wings'\": 1,\n",
       " 'costa': 1,\n",
       " 'oh': 109,\n",
       " 'breckinridge': 2,\n",
       " 'groaning': 1,\n",
       " 'tragi': 1,\n",
       " 'opposing': 2,\n",
       " 'dim': 1,\n",
       " 'bendan': 1,\n",
       " 'yadav': 1,\n",
       " 'victoires': 2,\n",
       " 'returns': 21,\n",
       " 'taut': 1,\n",
       " 'grapes': 2,\n",
       " 'senile': 3,\n",
       " 'tragedies': 2,\n",
       " 'ziegfeld': 1,\n",
       " 'bats': 1,\n",
       " 'swears': 1,\n",
       " 'array': 1,\n",
       " 'why': 414,\n",
       " 'grable': 3,\n",
       " \"noll's\": 1,\n",
       " 'wardrobes': 1,\n",
       " 'novac': 1,\n",
       " 'intellectually': 3,\n",
       " 'diverted': 1,\n",
       " 'beheaded': 2,\n",
       " 'roguish': 1,\n",
       " 'combining': 4,\n",
       " '101': 3,\n",
       " \"going'\": 1,\n",
       " 'havarti': 1,\n",
       " 'deliberately': 9,\n",
       " 'valérie': 1,\n",
       " \"'miracles'\": 1,\n",
       " 'stylophone': 1,\n",
       " 'churns': 1,\n",
       " 'last': 185,\n",
       " 'longest': 3,\n",
       " 'subject': 57,\n",
       " 'gawky': 1,\n",
       " 'foretells': 1,\n",
       " 'factories': 3,\n",
       " 'carl': 3,\n",
       " \"1940's\": 1,\n",
       " 'unexciting': 2,\n",
       " 'anarchic': 1,\n",
       " 'substance': 17,\n",
       " 'chix': 1,\n",
       " 'electric': 4,\n",
       " \"jackson's\": 5,\n",
       " 'aid': 5,\n",
       " 'shoved': 3,\n",
       " 'newborn': 1,\n",
       " \"series'\": 3,\n",
       " \"'cult\": 1,\n",
       " 'asking': 18,\n",
       " 'lunchtime': 1,\n",
       " 'auction': 2,\n",
       " 'franchot': 2,\n",
       " 'stare': 5,\n",
       " 'countdown': 1,\n",
       " 'sweetness': 4,\n",
       " 'documented': 2,\n",
       " 'expansive': 1,\n",
       " 'shaping': 1,\n",
       " 'lurid': 3,\n",
       " 'hearse': 1,\n",
       " \"'religious'\": 1,\n",
       " 'congratulations': 4,\n",
       " 'stripping': 1,\n",
       " 'spatially': 1,\n",
       " 'dealt': 3,\n",
       " 'mortuary': 1,\n",
       " 'trampling': 1,\n",
       " 'ignites': 2,\n",
       " 'proxy': 1,\n",
       " \"izzard's\": 2,\n",
       " 'nintendo': 3,\n",
       " 'mixture': 5,\n",
       " '19th': 4,\n",
       " 'derails': 1,\n",
       " 'sang': 3,\n",
       " \"corky's\": 1,\n",
       " 'smiley': 3,\n",
       " 'saint': 2,\n",
       " 'medieval': 6,\n",
       " 'marius': 1,\n",
       " 'bloch': 1,\n",
       " 'blond': 4,\n",
       " 'swabbie': 1,\n",
       " 'toxie': 1,\n",
       " 'julia': 17,\n",
       " 'amusing': 38,\n",
       " 'stabbed': 7,\n",
       " 'frosty': 2,\n",
       " 'complement': 3,\n",
       " 'inflated': 1,\n",
       " 'collect': 4,\n",
       " 'storm': 7,\n",
       " 'unpretentious': 1,\n",
       " \"80's\": 29,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained the tokenizer we can use it to vectorize the texts. In particular, we would like to transform the texts to sequences of word indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Find in the keras documentation the appropriate Tokenizer method to transform a list of texts to a sequence. Apply it to both the training and test data to obtain matrices **X_train** and **X_test**.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train=np.array(tokenizer.texts_to_sequences(texts_train))\n",
    "X_test=np.array(tokenizer.texts_to_sequences(texts_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now how a text has been transformed to a list of word indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84,\n",
       " 65,\n",
       " 35,\n",
       " 34,\n",
       " 684,\n",
       " 14,\n",
       " 8,\n",
       " 1,\n",
       " 88,\n",
       " 829,\n",
       " 5,\n",
       " 14,\n",
       " 8,\n",
       " 1,\n",
       " 334,\n",
       " 829,\n",
       " 123,\n",
       " 124,\n",
       " 35,\n",
       " 81,\n",
       " 12,\n",
       " 16,\n",
       " 42,\n",
       " 73,\n",
       " 152,\n",
       " 50,\n",
       " 3,\n",
       " 33,\n",
       " 5,\n",
       " 35,\n",
       " 121,\n",
       " 4,\n",
       " 9,\n",
       " 73,\n",
       " 1,\n",
       " 88,\n",
       " 829,\n",
       " 9,\n",
       " 13,\n",
       " 684,\n",
       " 14,\n",
       " 2,\n",
       " 12,\n",
       " 28,\n",
       " 4,\n",
       " 1,\n",
       " 158,\n",
       " 10,\n",
       " 447,\n",
       " 5,\n",
       " 61,\n",
       " 8,\n",
       " 1,\n",
       " 334,\n",
       " 829,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 315,\n",
       " 11,\n",
       " 17,\n",
       " 3,\n",
       " 630,\n",
       " 39,\n",
       " 4,\n",
       " 154,\n",
       " 2,\n",
       " 10,\n",
       " 315,\n",
       " 9,\n",
       " 3,\n",
       " 305,\n",
       " 39,\n",
       " 4,\n",
       " 154,\n",
       " 46,\n",
       " 1,\n",
       " 63,\n",
       " 27,\n",
       " 236,\n",
       " 5,\n",
       " 81,\n",
       " 15,\n",
       " 107,\n",
       " 18,\n",
       " 9,\n",
       " 124,\n",
       " 2,\n",
       " 12,\n",
       " 123,\n",
       " 10,\n",
       " 67,\n",
       " 315,\n",
       " 11,\n",
       " 17,\n",
       " 3,\n",
       " 359,\n",
       " 39,\n",
       " 154,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 12,\n",
       " 271,\n",
       " 36,\n",
       " 6,\n",
       " 20,\n",
       " 140,\n",
       " 11,\n",
       " 10,\n",
       " 83,\n",
       " 2,\n",
       " 104,\n",
       " 1,\n",
       " 88,\n",
       " 829,\n",
       " 308]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is enough to train a Sequential Network. However, for efficiency reasons it is recommended that all sequences in the data have the same number of elements. Since this is not the case for our data, should **pad** the sequences to ensure the same length. The padding procedure adds a special *null* symbol to short sequences, and clips out parts of long sequences, thus enforcing a common size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Find in the keras documentation the appropriate text preprocessing method to pad a sequence. Then pad all sequences to have a maximum of 300 words, both in the training and test data.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_pad = pad_sequences(X_train, maxlen=300)\n",
    "X_test_pad = pad_sequences(X_test, maxlen=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure indexes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to build a model based just on word indexes. Since keras expects sequential inputs as 3-dimensional array with dimensions NUMBER_SEQUENCES x SEQUENCE_LENGTH x FEATURES and we will we using only the indexes, our features dimension is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Create new variables **X_train_idx** and **X_test_idx**, reshaped versions of **X_train** and **X_test**, in which each index has been transformed into a 1-element list.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_idx = np.reshape(X_train_pad, (len(X_train_pad),300,1))\n",
    "X_test_idx = np.reshape(X_test_pad, (len(X_test_pad),300,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Build, compile and train a keras network with an LSTM layer of 32 units and dropout 0.9, followed by a Dense layer of 1 unit with sigmoid activation. Use the binary crossentroy loss function for training, together with the adam optimizer. Train for 10 epochs. After training, measure the accuracy on the test set.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.8937 - acc: 0.4960     \n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 7s - loss: 0.8669 - acc: 0.4981     \n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 7s - loss: 0.8643 - acc: 0.4933     \n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 7s - loss: 0.8510 - acc: 0.4869     \n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 7s - loss: 0.8329 - acc: 0.4981     \n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 7s - loss: 0.7821 - acc: 0.5221     \n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 7s - loss: 0.7877 - acc: 0.5051     \n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s - loss: 0.7798 - acc: 0.4939     \n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 7s - loss: 0.7863 - acc: 0.4976     \n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 7s - loss: 0.7460 - acc: 0.5120     \n",
      "625/625 [==============================] - 0s     \n",
      "Test score: 0.695407935905\n",
      "Test accuracy: 0.484799999666\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,LSTM,Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X_train_idx.shape[1], X_train_idx.shape[2])))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_idx, y_train, batch_size=128, nb_epoch=10)\n",
    "score, acc = model.evaluate(X_test_idx, y_test,batch_size=128)\n",
    "\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning an embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using indexes as a representation of words is a very poor approach. We can easily improve over that by using an **Embedding** layer at the very beginning of the network. This layer will transform word indexes to a vector representation that is learned with the model together with the rest of network weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Create a new network similar to the previous one, but adding an Embedding as the first layer of the network. Configure the Embedding layer to produce a vector representation of 64 elements. Then train the network with similar setting to the previous one. Has the test accuracy improved?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/exclamation.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "The Embedding layer accepts lists of indexes as inputs, so you don't need to use the **X_train_idx** representation you created for the previous network.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 9s - loss: 0.6943 - acc: 0.4949     \n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.6816 - acc: 0.5904     \n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.6560 - acc: 0.6357     \n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.5851 - acc: 0.7355     \n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.5469 - acc: 0.7456     \n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.5509 - acc: 0.7600     \n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.4804 - acc: 0.8085     \n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.4464 - acc: 0.8331     \n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.4177 - acc: 0.8507     \n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 8s - loss: 0.3652 - acc: 0.8800     \n",
      "625/625 [==============================] - 1s     \n",
      "Test score: 0.471828553772\n",
      "Test accuracy: 0.78399999876\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000,64))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_pad, y_train, batch_size=128, nb_epoch=10)\n",
    "score, acc = model.evaluate(X_test_pad, y_test,batch_size=128)\n",
    "\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like other neural layers, LSTM layers can be stacked on top of each other to produce more complex models. Care must be taken, however, that the LSTM layers before the last one generate a whole sequence of outputs for the following LSTM to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Repeat the training of the previous network, but using 2 LSTM layers. Make sure to configure the first LSTM layer in a way that it outputs a whole sequence for the next layer.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 19s - loss: 0.6928 - acc: 0.5456    \n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 16s - loss: 0.6879 - acc: 0.5435    \n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 16s - loss: 0.6729 - acc: 0.5851    \n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 17s - loss: 0.6067 - acc: 0.6720    \n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 17s - loss: 0.5604 - acc: 0.7141    \n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 17s - loss: 0.4886 - acc: 0.7856    \n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 17s - loss: 0.4403 - acc: 0.8187    \n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 16s - loss: 0.4518 - acc: 0.8139    \n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 16s - loss: 0.3973 - acc: 0.8469    \n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 16s - loss: 0.3488 - acc: 0.8731    \n",
      "625/625 [==============================] - 1s     \n",
      "Test score: 0.522992933273\n",
      "Test accuracy: 0.775999997425\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000,64))\n",
    "model.add(LSTM(32,return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_pad, y_train, batch_size=128, nb_epoch=10)\n",
    "score, acc = model.evaluate(X_test_pad, y_test,\n",
    "                            batch_size=128)\n",
    "\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
