{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de sentimientos con reviews de productos de Amazon España (opcional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si has hecho ya el ejercicio de web scraping con `Requests` y `BeautifulSoup` habrás visto cómo extraer datos de una página web.\n",
    "\n",
    "El dataset que utilizarás en este ejercicio (que no es obligatorio entregar) lo he generado utilizando `Scrapy` y `BeautifulSoup`, y contiene unas $700.000$ entradas con dos columnas: el número de estrellas dadas por un usuario a un determinado producto y el comentario sobre dicho producto; exactamente igual que en el ejercico de scraping.\n",
    "\n",
    "Ahora, tu objetivo es utilizar técnicas de procesamiento de lenguaje natural para hacer un clasificador que sea capaz de distinguir (¡y predecir!) si un comentario es positivo o negativo.\n",
    "\n",
    "Es un ejercicio MUY complicado, más que nada porque versa sobre técnicas que no hemos visto en clase. Así que si quieres resolverlo, te va a tocar estudiar y *buscar por tu cuenta*; exactamente igual que como sería en un puesto de trabajo. Dicho esto, daré un par de pistas:\n",
    "\n",
    "+ El número de estrellas que un usuario da a un producto es el indicador de si a dicho usuario le ha gustado el producto o no. Una persona que da 5 estrellas (el máximo) a un producto probablemente esté contento con él, y el comentario será por tanto positivo; mientras que cuando una persona da 1 estrella a un producto es porque no está satisfecha... \n",
    "+ Teniendo el número de estrellas podríamos resolver el problema como si fuera de regresión; pero vamos a establecer una regla para convertirlo en problema de clasificación: *si una review tiene 4 o más estrellas, se trata de una review positiva; y será negativa si tiene menos de 4 estrellas*. Así que probablemente te toque transformar el número de estrellas en otra variable que sea *comentario positivo/negativo*.\n",
    "\n",
    "Y... poco más. Lo más complicado será convertir el texto de cada review en algo que un clasificador pueda utilizar y entender (puesto que los modelos no entienden de palabras, sino de números). Aquí es donde te toca investigar las técnicas para hacerlo. El ejercicio se puede conseguir hacer, y obtener buenos resultados, utilizando únicamente Numpy, pandas y Scikit-Learn; pero siéntete libre de utilizar las bibliotecas que quieras.\n",
    "\n",
    "Ahora escribiré una serie de *keywords* que probablemente te ayuden a saber qué buscar:\n",
    "\n",
    "`bag of words, tokenizer, tf, idf, tf-idf, sklearn.feature_extraction, scipy.sparse, NLTK (opcional), stemmer, lemmatizer, stop-words removal, bigrams, trigrams`\n",
    "\n",
    "No te desesperes si te encuentras muy perdido/a y no consigues sacar nada. Tras la fecha de entrega os daré un ejemplo de solución explicado con todo el detalle posible.\n",
    "\n",
    "¡Ánimo y buena suerte!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respuesta\n",
    "He estado mirando por internet como hacer este problema.\n",
    "Vamos a crear una matriz donde las filas sean los comentarios y las columnas, las palabras mas repetidas.\n",
    "En cada celda de la matriz, obtendremos las veces que esa palabra se repite.\n",
    "Esta matriz, constituirá nuestras _features_ y el _label_ será una columna de 1 y 0 donde 1 significa comentario positivo y 0 negativo.\n",
    "Para realizar esto necesitamos:\n",
    "\n",
    "* limpiar cada comentario de caracteres que no son letras\n",
    "* crear un \"Word of Bag\", donde ponemos las ocurrencias de cada palabra y el impacto (numero de ocurrencias)\n",
    "* Elegir algun clasificador que funcione bien con lenguaje natural (RandomForests,Naive-Bayes..)\n",
    "* hacer la prediccion y el posterios _score_\n",
    "\n",
    "Me he basado en este tutorial de Kaggle:\n",
    "\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "\n",
    "Y viendo que el tamaño de las matriz del WordOfBags puede ser enorme (unos 5.5 gigas para 1000 palabras), he tenido que limitar el numero de palabras a contar a 300 para llegar a buen puerto.\n",
    "\n",
    "## procesando el dataset.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=pd.read_csv(\"amazon_es_reviews.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#añadimos una columna \"positivo\" y mapeamos los comentarios a 1 o 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset[\"positivo\"]=dataset.apply(lambda x: int(1) if x[\"estrellas\"]>3 else int(0), axis=1)\n",
    "\n",
    "dataset[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creamos un nuevo dataset solo con las columnas comentario y positivo\n",
    "ds=dataset\n",
    "del ds[\"estrellas\"]\n",
    "ds[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vamos a ver que pinta tienen los comentarios, por si hay que pasarle algun parser, imprimimos algunos de ellos..\n",
    "print(ds[\"comentario\"][0])\n",
    "print(ds[\"comentario\"][40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solo tenemos carateres del tipo \",(;).\" habrá que eliminarlos para tener un WordOfBags (WOB) limpio.\n",
    "\n",
    "Tendremos que decidir que hacemos con las palabras que no tienen un significado \"sentimental\", es decir, aquellas que no conllevan juicios de valor. Para ello, existe una libreria en Python que trae listas de \"stop Words\" en cualquier idioma. Para ello, debemos importar la librearia NTLK (Python Natural Language Toolkit y acceder al metodo: stop_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download()  esto solo hay que hacerlo una vez\n",
    "stopwords_sp=stopwords.words(\"spanish\")  #lista de stopwords en español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando las features desde el Bag of words\n",
    "\n",
    "Ahora que tenemos los comentarios, la pregunta es:¿cómo los convertimos en alguna representación matemática compatible con Machine Learning?\n",
    "Se podría contar el número de veces que una palabra se repite, por ejemplo:\n",
    "\n",
    "\"el raton está en la cocina\"\n",
    "\"el gato persigue al raton\"\n",
    "\n",
    "de estas dos frases, nuestro vocabulario sería:\n",
    "{ el, ratón, está, en, la, cocina,gato, persigue, al }\n",
    "\n",
    "contando las veces que aparece cada palabra en cada frase, tendriamos los siguientes ocurrencias:\n",
    "\n",
    "{2,2,1,1,1,1,1,1,1} (\"el\" aparece 2 veces, \"ratón\" otas 2 y asi..)\n",
    "\n",
    "De esta forma, podrímos quedarnos con las palabras que mas se repiten en nuestros comentarios, para ello vamos a crear los VectorizeCounters:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "MAX_WORDS=300\n",
    "# inicializamos el CountVectorizer, con un maximo de MAX_WORDS palabras\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             ngram_range=(1, 2),\n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = stopwords_sp,   \n",
    "                             max_features = MAX_WORDS) #me quedo con las MAX_WORDS palabras que mas se repiten\n",
    "\n",
    "# y finalmente creamos un array de features, que tiene tantas filas como comentarios y en cada columna\n",
    "# el numero de veces que aparece cada palabra de las 100 mas repetidas\n",
    "train_data_features = vectorizer.fit_transform(ds[\"comentario\"])\n",
    "print(type(train_data_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos a ver que palabras contiene nuestro vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print (vocab[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dist = np.sum(train_data_features.toarray(), axis=0)\n",
    "\n",
    "#vamos a imprimir cada palabra unica y el numero de ocurrencias\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print ( count, tag )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entrenando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(train_data_features))\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos una matriz de 702446 filas y 100 columnas. He tenido que restringir el numero de palabras a 100 puesto que con 5000 se va de memoria. \n",
    "Nos queda unicamente concatenar a esta matriz, el vector de positivo/negativo y llamar a algun clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#como el train_data_features es una matriz disperas, usamos los metodos de la libreria scipy.sparse para concatenar ambas matrices\n",
    "vector_positivo=csr_matrix(ds[\"positivo\"]).T\n",
    "\n",
    "sp_dataset=sp.hstack([train_data_features,vector_positivo]).toarray()\n",
    "#comprobamos que lo hemos hecho bien, observando la forma de cada matriz\n",
    "print(vector_positivo.shape)\n",
    "print(train_data_features.shape)\n",
    "print(sp_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datos_spliteados = train_test_split(sp_dataset,\n",
    "                                    train_size=0.8, # 80% training\n",
    "                                    test_size=0.2   # 20% testing\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#separamos los datos en train y test\n",
    "train=datos_spliteados[0]\n",
    "test=datos_spliteados[1]\n",
    "\n",
    "train_label=train[:,-1]\n",
    "test_label=test[:,-1]\n",
    "\n",
    "train_features= np.delete(train, MAX_WORDS, axis=1)\n",
    "test_features=np.delete(test,MAX_WORDS,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# probamos ahora con un Naives-Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(test_features, test_label)\n",
    "print(\"bien clasificados en test: %.16f\" % gnb.score(X=test_features,y=test_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Probamos con un RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "forest = forest.fit( train_features, train_label )\n",
    "print(\"Bien clasificados en test: %.16f\" % forest.score(X=test_features,y=test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
